{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seanjie250/UCSB_CS190I/blob/main/linear_regression_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression with Python"
      ],
      "metadata": {
        "id": "L8kwDo1l4GLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this machine problem (MP), you will implement a simple linear regression model in Python. You will create your own dataset and visualize the results using Matplotlib.\n",
        "\n"
      ],
      "metadata": {
        "id": "xd23PwWB2BzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reset_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "metadata": {
        "id": "ScZSQPMJ5qLF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Data Preparation**\n",
        "\n",
        "Write a Python function named `generate_data` that takes the following parameters:\n",
        "\n",
        "1.   `num_examples`: representing the number of data points to generate.\n",
        "2.   `input_dim`: representing the dimensionality of data points to generate (without the bias dimension)\n",
        "\n",
        "Inside the function:\n",
        "\n",
        "1.   Specify the linear coefficient w and bias term (generate them randomly).\n",
        "2.   Generate `num_examples` random values with dimension `input_dim`.\n",
        "3.   Calculate the corresponding target variable `y`.\n",
        "4.   (**opition**) Add random noise to `y` using a normal distribution with mean 0 and a pre-specified standard deviation.\n",
        "\n",
        "Return:\n",
        "\n",
        "1.   `data`: shape (num_examples, input_dim + 1), where the first dimension is the additional dimension and has value of 1.\n",
        "2.   `y`: the target variable."
      ],
      "metadata": {
        "id": "zczGtHPCTVXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_data(num_examples, input_dim):\n",
        "    \"\"\"\n",
        "    This function generates a dataset for linear regression.\n",
        "    \"\"\"\n",
        "    data, gt_y = None, None\n",
        "\n",
        "    # Generate random linear coefficient and bias term\n",
        "    w = np.random.randn(input_dim)\n",
        "    b = np.random.randn()\n",
        "\n",
        "    # Generate random input data\n",
        "    X = np.random.randn(num_examples, input_dim)\n",
        "    # Add bias term (1) to the data\n",
        "    data = np.hstack((np.ones((num_examples, 1)), X))\n",
        "\n",
        "    # Calculate target variable\n",
        "    y = np.dot(data, np.concatenate(([b], w))) + np.random.normal(0, 0.1, size=num_examples)\n",
        "\n",
        "    return data, y\n",
        "\n"
      ],
      "metadata": {
        "id": "lgDmpoN-38t0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Data Visualization**\n",
        "\n",
        "Generate synthetic data using your `generate_data` function.\n",
        "Create scatter plots to visualize the generated data points by setting input_dim to 1.\n"
      ],
      "metadata": {
        "id": "w7GFJOLbcHXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_example = 20\n",
        "input_dim = 1\n",
        "reset_seed(1)\n",
        "\n",
        "data, gt_y = generate_data(num_example, input_dim)\n",
        "\n",
        "plt.scatter(data[:,1], gt_y, label='Data Points', color='blue', marker='o')\n",
        "plt.xlabel('data')\n",
        "plt.ylabel('gt_y')\n",
        "plt.title('Scatter plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aGpHOvINaor2",
        "outputId": "09bf5749-5b5d-4d9e-94a1-477cd4667ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Data Splitting for Model Training**\n",
        "\n",
        "Split the created dataset into a training set and a validation set. Please use the variable `train_perc` to control the proportion of data used for training. Splitting data into separate sets is crucial for evaluating machine learning models.\n",
        "\n",
        "Inside the function:\n",
        "\n",
        "1.   Calculate the number of samples to include in the training set and the validation set based on the `train_perc` parameter, which is a floating-point number between 0 and 1, indicating the proportion of data to be used for training. For example, if `train_perc` = 0.8, 80% of the data will be used for training, and the remaining 20% will be used for validation.\n",
        "\n",
        "2.   Randomly shuffle the data to ensure that it's not sorted in any particular order.\n",
        "\n",
        "3.   Split the data into training and validation sets according to the specified proportions.\n",
        "\n"
      ],
      "metadata": {
        "id": "0gsVqgW0mYGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fAQh7jLS19Re"
      },
      "outputs": [],
      "source": [
        "def train_test_split(data, gt_y, train_perc):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and validation set.\n",
        "    \"\"\"\n",
        "    assert(train_perc > 0. and train_perc <= 1.)\n",
        "\n",
        "    num_samples = data.shape[0]\n",
        "    num_train_samples = int(num_samples * train_perc)\n",
        "\n",
        "    # Randomly shuffle the data\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    data = data[indices]\n",
        "    gt_y = gt_y[indices]\n",
        "\n",
        "    # Split data and gt_y into train and val\n",
        "    data_train = data[:num_train_samples]\n",
        "    y_train = gt_y[:num_train_samples]\n",
        "    data_test = data[num_train_samples:]\n",
        "    y_test = gt_y[num_train_samples:]\n",
        "\n",
        "    return data_train, y_train, data_test, y_test\n",
        "    reset_seed(1)\n",
        "    train_perc = 0.3\n",
        "    data_train, y_train, data_test, y_test = train_test_split(data, gt_y, train_perc)\n",
        "    print(data_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Model Training - Analytically**\n",
        "\n",
        "You need to implement a simple linear regression function to obtian optimal weight and bias using the analytical solution method. This method allows you to calculate the coefficients of the linear regression model directly without iterative optimization algorithms."
      ],
      "metadata": {
        "id": "YJYcP_uUnTH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_reg_analyt(X, y):\n",
        "    \"\"\"\n",
        "    Train linear regression analytical.\n",
        "    Optimial solution:\n",
        "    (X^T * X)^{-1} * X^T * y\n",
        "    \"\"\"\n",
        "    W_optim = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "    return W_optim"
      ],
      "metadata": {
        "id": "uUTgD8ff53Or"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Train Your Model**\n",
        "\n",
        "Now, you already have all the helper functions to train your linear regression model. Obtain the predicted parameters."
      ],
      "metadata": {
        "id": "NFqskhvUqTMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = lin_reg_analyt(data_train, y_train)\n",
        "print(parameters)"
      ],
      "metadata": {
        "id": "Aa8cw5R76MFU",
        "outputId": "aee298fa-5720-4ce2-eacb-5f5dd7d0d6f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.60298134  1.67033723]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: Model Evaluation**\n",
        "Make predictions on the validation data using your trained model.  Calculate and display the Mean Squared Error (MSE) score to evaluate the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qS0C9_dp0MH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "y_test_pred = np.dot(data_test, parameters)\n",
        "\n",
        "\n",
        "# Calculate MSE error\n",
        "mse = np.mean((y_test - y_test_pred) ** 2)\n",
        "print(f\"MSE on validation set: {mse}\")"
      ],
      "metadata": {
        "id": "MWDqYtlf6hfg",
        "outputId": "94646e92-5a24-44fe-cacd-2834bad0b8aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE on validation set: 0.007992390249793641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remark**\n",
        "\n",
        "By now, you have finished the basic version of linear regression. With no additive noise in your data generation function, the MSE you achieve by using the analytical solution should be almost zero."
      ],
      "metadata": {
        "id": "kfkSzjuxt84D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework (Due on Sat Oct 21, 11:59pm PST)"
      ],
      "metadata": {
        "id": "7kAUMTJajo6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Basic Gradient Descent [30 points]**\n",
        "\n",
        "Now, instead of using the analytic method to find the best parameters for the linear regression problem, you need to implement a gradient descent (GD) approach. Initialize your parameters with all zeros, set the number of iterations to 1000, and the learning rate to 1e-3. Report the MSE score and compare it to the one you found using the analytical approach."
      ],
      "metadata": {
        "id": "e1aHm3l_vUU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gd_linear_regression(X, y, num_step, lr):\n",
        "    \"\"\"Use gradient descent to find the optimal parameters\"\"\"\n",
        "    # initialize parameters\n",
        "    parameters = np.zeros((X.shape[1], 1))\n",
        "\n",
        "    m = len(y)\n",
        "\n",
        "    for step in range(num_step):\n",
        "        # Compute predictions\n",
        "        predictions = np.dot(X, parameters)\n",
        "\n",
        "        # Compute error\n",
        "        error = predictions - y\n",
        "\n",
        "        # Compute gradient\n",
        "        gradient = np.dot(X.T, error) / m\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = parameters - lr * gradient\n",
        "\n",
        "    return parameters\n",
        "\n",
        "num_step = 1000\n",
        "lr = 1e-3\n",
        "parameters_gd = gd_linear_regression(data_train, y_train, num_step, lr)\n",
        "print(parameters_gd)\n"
      ],
      "metadata": {
        "id": "HasYEcAwwZZL",
        "outputId": "cee48d31-8650-4318-8ef0-df2a2d6ae19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.73881146 -0.2338053  -0.07457563 -0.74639792  0.54796403 -0.62204678]\n",
            " [ 0.42034724  0.03588396  0.01144572  0.11455564 -0.08410041  0.09547048]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now evaluate the parameters you get by gradient descent on the validation set."
      ],
      "metadata": {
        "id": "OQLhTSeEIsJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_test_reshaped = data_test.reshape(-1, 2)\n",
        "\n",
        "y_test_pred = np.dot(data_test_reshaped, parameters_gd)\n",
        "##################################################\n",
        "## TODO: Make predictions on the validation set ##\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "################ End of your code ################\n",
        "##################################################\n",
        "\n",
        "\n",
        "\n",
        "mse = np.mean((y_test_reshaped - y_test_pred) ** 2)\n",
        "print(f\"MSE on validation set: {mse}\")"
      ],
      "metadata": {
        "id": "au2_Q9gUI2r5",
        "outputId": "2236bfea-cf1f-418d-85c3-25ca95393041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-372c5b03f03a>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m##################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_test_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_reshaped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2: Learning Rate in Gradient Descent [20 points]**\n",
        "\n",
        "In this problem, you need to experiment with different learning rates. Please report the MSE score when you set your learning rate as 0.05 and 1e-7 (with the number of gradient descent steps remaining at 1000). Discuss your findings."
      ],
      "metadata": {
        "id": "Mx6xeK-h2J_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " MSE = 0.007992390249793641 when lr is 0.05;\n",
        " MSE ="
      ],
      "metadata": {
        "id": "FO8G2ngerozA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Linear Regression with L2 Regularization [50 points]**\n",
        "\n",
        " Recall the objective function for linear regression can be expressed as $E(\\mathbf{w})=\\frac{1}{N}\\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\|^2$. Minimizing this function with respect to $\\mathbf{w}$ leads to the optimal $\\mathbf{w}^*$ as $(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.  However, this solution holds only when $\\mathbf{X}^T\\mathbf{X}$ is nonsingular.\n",
        "\n",
        "To overcome this problem, the following objective\n",
        "function is commonly minimized instead:\n",
        "$E_2(\\mathbf{w})=\\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\|^2+\\alpha\\|\\mathbf{w}\\|^2,$ where $\\alpha>0$ is a user-specified parameter.  This objective function is often called ridge regression.  \n",
        "\n",
        "Now, you need to repeat what you have done to obtain the optimal parameters of the ridge regression model using **both analytical** and **gradient descent** approaches.\n",
        "\n",
        "Compare the MSE on the validation set for parameters obtained by four method: linear regression with analytical and gradient descent solution, and ridge regression with analytical and gradient descent solution (feel free to explore different values for $\\alpha$).\n",
        "\n",
        "Report your results in the following setting and discuss your findings:\n",
        "\n",
        "*   Generate 1000 random data points with input dimension 100 (not including bias). Fix 100 of them as test points. Among the remaining 900 data points, use different number of data points (25, 50, 75, 100, ..., 300) as training data. Report MSE on the test set and plot the MSE as a function of the number of training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "hLSag1cK32Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_regression_analyt(X, y, alpha):\n",
        "    \"\"\"Train ridge regression analytically.\"\"\"\n",
        "    W_optim = None\n",
        "    ##########################################\n",
        "    ## TODO: Calculate the optimal solution ##\n",
        "    ##########################################\n",
        "\n",
        "    ##########################################\n",
        "    ############ End of your code ############\n",
        "    ##########################################\n",
        "    return W_optim"
      ],
      "metadata": {
        "id": "QoULzFNb6feW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gd_ridge_regression(X, y, alpha, num_step, lr):\n",
        "    \"\"\"Use gradient descent to find the optimal parameters\"\"\"\n",
        "    # initialize parameters\n",
        "    parameters = np.zeros((X.shape[1], 1))\n",
        "    #############################################\n",
        "    ## TODO: Use GD to find optimal parameters ##\n",
        "    #############################################\n",
        "\n",
        "    #############################################\n",
        "    ########### End of your code ################\n",
        "    #############################################\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "6V3Cgtl8K4XY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}